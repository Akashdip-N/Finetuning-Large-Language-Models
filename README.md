# Fine-Tuning Large Language Models (LLMs)
This repository contains Jupyter notebooks from the **Fine-Tuning Large Language Models (LLMs)** short course offered by DeepLearning which you could get here from this [link](https://learn.deeplearning.ai/courses/finetuning-large-language-models).

## 📚 Course Overview
The course covers key aspects of fine-tuning LLMs (Large Language Models) including instruction tuning, data preparation, training, and evaluation.

## 📖 Course Topics
The course is structured into six modules, each represented by a Jupyter notebook:

1. **Why Finetuning.ipynb** - Introduction to the importance and need for finetuning LLMs.
2. **Finetuning Overview.ipynb** - Overview of finetuning methods and approaches.
3. **Instruction Tuning.ipynb** - Techniques for instruction tuning in LLMs.
4. **Data Preparation.ipynb** - Preparing and formatting datasets for effective finetuning.
5. **Training LLM.ipynb** - Implementing finetuning on large language models.
6. **Evaluation.ipynb** - Evaluating the performance of fine-tuned models.

## 🔧 Setup & Installation
To run these notebooks locally, follow these steps:

1. Clone the repository:
```bash
   git clone https://github.com/Akashdip-N/Finetuning-Large-Language-Models.git
   cd Finetuning-Large-Language-Models
```

## 🛠 Contributions
Contributions are welcome! For feature requests, bug reports, or questions, please raise an issue on the repository.

## 📄 License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🏆 Acknowledgements
*	[Lamini](https://www.lamini.ai/) for contributions to the course content.
*	[DeepLearning.AI](https://learn.deeplearning.ai/) for the educational material.
* [Hugging Face](https://huggingface.co/) for the Transformers library.